{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploration and Preprocessing Patrick"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import relevant packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ARIVE | Your favorite brandsGet the appOur Vis...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Rückabwicklung - von Lebensversicherungen\\nWir...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\\nStarke Versicherungen für deinen Lifestyle...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\\nRESTUBE - the airbag for more freedom and s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CureVac - Wir revolutionieren die mRNA für das...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3275</th>\n",
       "      <td>\\n                LEMONSGATE               ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3276</th>\n",
       "      <td>Safily\\n \\n√úberspringen\\nSuchen\\nSuche schlie...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3277</th>\n",
       "      <td>Startseite | ICO-LUX\\n \\nFraud Prevention\\nÜbe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3278</th>\n",
       "      <td>Versus | Compare everything\\nCategoriessmartph...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3279</th>\n",
       "      <td>Selfstorage München &amp; deutschlandweit | Storag...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3280 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Content\n",
       "0     ARIVE | Your favorite brandsGet the appOur Vis...\n",
       "1     Rückabwicklung - von Lebensversicherungen\\nWir...\n",
       "2       \\nStarke Versicherungen für deinen Lifestyle...\n",
       "3      \\nRESTUBE - the airbag for more freedom and s...\n",
       "4     CureVac - Wir revolutionieren die mRNA für das...\n",
       "...                                                 ...\n",
       "3275     \\n                LEMONSGATE               ...\n",
       "3276  Safily\\n \\n√úberspringen\\nSuchen\\nSuche schlie...\n",
       "3277  Startseite | ICO-LUX\\n \\nFraud Prevention\\nÜbe...\n",
       "3278  Versus | Compare everything\\nCategoriessmartph...\n",
       "3279  Selfstorage München & deutschlandweit | Storag...\n",
       "\n",
       "[3280 rows x 1 columns]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# List the folders containing the text files\n",
    "folders = ['../data/raw/txt_files_0_1750', '../data/raw/txt_files_1751_3500', '../data/raw/txt_files_3501_5000']\n",
    "\n",
    "# Initialize an empty list to store the data\n",
    "data = []\n",
    "\n",
    "# Iterate through each folder\n",
    "for folder in folders:\n",
    "    # Get the list of text files in the folder\n",
    "    file_list = os.listdir(folder)\n",
    "    \n",
    "    # Iterate through each text file\n",
    "    for file_name in file_list:\n",
    "        # Read the contents of the file\n",
    "        file_path = os.path.join(folder, file_name)\n",
    "        try:\n",
    "            with open(file_path, 'r') as file:\n",
    "                content = file.read()\n",
    "                # Append content to the data list\n",
    "                data.append(content)\n",
    "        except FileNotFoundError:\n",
    "            print(f\"File {file_name} not found.\")\n",
    "        except IOError:\n",
    "            print(f\"Error reading the file {file_name}.\")\n",
    "        \n",
    "# Create a DataFrame from the data list\n",
    "df = pd.DataFrame(data, columns=['Content'])\n",
    "df\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocesssing Functions from the exercise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import packages\n",
    "import re\n",
    "\n",
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "\n",
    "# import matplotlib.pyplot as plt\n",
    "# import seaborn as sns\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "# from skmultilearn.problem_transform import BinaryRelevance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define functions\n",
    "def cleanHtml(sentence):\n",
    "    cleanr = re.compile('<.*?>')\n",
    "    cleantext = re.sub(cleanr, ' ', str(sentence))\n",
    "    return cleantext\n",
    "\n",
    "def cleanPunc(sentence): #function to clean the word of any punctuation or special characters\n",
    "    cleaned = re.sub(r'[?|!|\\'|\"|#]',r'', sentence)\n",
    "    cleaned = re.sub(r'[.|,|)|(|\\|/]',r' ', cleaned)\n",
    "    cleaned = cleaned.strip()\n",
    "    cleaned = cleaned.replace(\"\\n\",\" \")\n",
    "    return cleaned\n",
    "\n",
    "def keepAlpha(sentence):\n",
    "    alpha_sent = \"\"\n",
    "    for word in sentence.split():\n",
    "        alpha_word = re.sub('[^a-z A-Z]+', ' ', word)\n",
    "        alpha_sent += alpha_word\n",
    "        alpha_sent += \" \"\n",
    "    alpha_sent = alpha_sent.strip()\n",
    "    return alpha_sent\n",
    "\n",
    "def removeStopWords(sentence, stopwords):\n",
    "    re_stop_words = re.compile(r\"\\b(\" + \"|\".join(stop_words) + \")\\\\W\", re.I)   \n",
    "    return re_stop_words.sub(\" \", sentence)\n",
    "\n",
    "def stemming(sentence):\n",
    "    stemSentence = \"\"\n",
    "    for word in sentence.split():\n",
    "        stem = stemmer.stem(word)\n",
    "        stemSentence += stem\n",
    "        stemSentence += \" \"\n",
    "    stemSentence = stemSentence.strip()\n",
    "    return stemSentence\n",
    "\n",
    "# define stopwords\n",
    "stop_words = set(stopwords.words(['german', 'english']))\n",
    "\n",
    "# define stemmer\n",
    "stemmer = SnowballStemmer('german', 'english')\n",
    "\n",
    "# apply functions\n",
    "df['Content'] = df['Content'].str.lower()\n",
    "df['Content'] = df['Content'].apply(cleanHtml)\n",
    "df['Content'] = df['Content'].apply(cleanPunc)\n",
    "df['Content'] = df['Content'].apply(keepAlpha)\n",
    "df['Content'] = df['Content'].apply(removeStopWords, stopwords=stopwords)\n",
    "df['Content'] = df['Content'].apply(stemming)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ariv favorit brandsget appour visionpartnershi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>r ckabwickl lebensversicher hol doppelt leb re...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>stark versicher f r lifestyl held de hom produ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>restub airbag freedom safety wat restub skip c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>curevac revolutioni mrna f r leb mensch ber be...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             Content\n",
       "0  ariv favorit brandsget appour visionpartnershi...\n",
       "1  r ckabwickl lebensversicher hol doppelt leb re...\n",
       "2  stark versicher f r lifestyl held de hom produ...\n",
       "3  restub airbag freedom safety wat restub skip c...\n",
       "4  curevac revolutioni mrna f r leb mensch ber be..."
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check result\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for tokenization -> only use when vectorizer doesn't include tokenization (probably included)\n",
    "#def tokenize(text):\n",
    "#    tokens = re.split(r'\\W+', text)\n",
    "\n",
    "#    return tokens\n",
    "\n",
    "# applying function to the column\n",
    "#df['Content'] = df['Content'].apply(lambda x: tokenize(x))\n",
    "#df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specifiy the path to the folder where you want to save the CSV file\n",
    "folder_path = '../data/preprocessed'\n",
    "\n",
    "# Specify the file name and full path for the CSV file\n",
    "file = 'preprocessed_data.csv'\n",
    "path = os.path.join(folder_path, file)\n",
    "\n",
    "# Save the preprocessed data to a CSV file\n",
    "df.to_csv(path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate requirements.txt without some weird paths instead of the package version\n",
    "!pip list --format=freeze > requirements_preprocessing.txt"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
