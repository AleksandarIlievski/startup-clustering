{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vyZNWSAuZsNr"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "import re\n",
        "from urllib.parse import urlparse"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2dXXCEGTa-EW"
      },
      "outputs": [],
      "source": [
        "# path to csv file\n",
        "data = pd.read_csv(\"/content/drive/MyDrive/\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WEmwVKcHbGU_"
      },
      "outputs": [],
      "source": [
        "data.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J065garRZ_mW"
      },
      "outputs": [],
      "source": [
        "def is_scraping_allowed(url):\n",
        "    \"\"\"\n",
        "    Checks if scraping is allowed for the given URL.\n",
        "\n",
        "    Args:\n",
        "        url (str): The URL to check.\n",
        "\n",
        "    Returns:\n",
        "        bool: True if scraping is allowed, False otherwise.\n",
        "    \"\"\"\n",
        "    # Parse the URL to extract the domain.\n",
        "    domain = urlparse(url).netloc\n",
        "\n",
        "    # Get the URL for the robots.txt file for the domain.\n",
        "    robots_url = f\"https://{domain}/robots.txt\"\n",
        "\n",
        "    # Send a GET request to the robots.txt file.\n",
        "    try:\n",
        "        response = requests.get(robots_url)\n",
        "    except:\n",
        "        # If there was an error, assume scraping is not allowed.\n",
        "        return False\n",
        "\n",
        "    # Check if the status code of the response is OK.\n",
        "    if response.status_code != 200:\n",
        "        # If not, assume scraping is not allowed.\n",
        "        return False\n",
        "\n",
        "    # Parse the robots.txt file.\n",
        "    robots_txt = response.text\n",
        "\n",
        "    # Check if the user-agent \"*\" is disallowed from scraping any pages.\n",
        "    #if \"User-agent: *\\nDisallow: /\" in robots_txt:\n",
        "    #    return False\n",
        "\n",
        "    # Check if the user-agent \"*\" is specifically disallowed from scraping the given URL.\n",
        "    if f\"User-agent: *\\nDisallow: {url}\" in robots_txt:\n",
        "        return False\n",
        "\n",
        "    # If none of the above conditions are met, assume scraping is allowed.\n",
        "    return True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jQ1KAgIaU1kZ"
      },
      "outputs": [],
      "source": [
        "def getWebsiteText(url):\n",
        "    # Headers with Accept-Language set to English\n",
        "    headers = {\n",
        "        'Accept-Language': 'en-US,en;q=0.1',\n",
        "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3'\n",
        "    }\n",
        "\n",
        "    # Send a request to the website and get the response\n",
        "    try:\n",
        "        response = requests.get(url, headers=headers)\n",
        "        if response.status_code == 200:\n",
        "            #print(f\"The link {url} is working.\")\n",
        "            # Parse the HTML content with BeautifulSoup\n",
        "            soup = BeautifulSoup(response.content, 'html.parser')\n",
        "            text = soup.getText()\n",
        "            return text\n",
        "        else:\n",
        "            return f\"Error: {response.status_code}\"\n",
        "\n",
        "    except Exception as e:\n",
        "        # Handle the error here\n",
        "        if 'response' not in locals():\n",
        "            return \"Error: Failed to get response\"\n",
        "        return f\"Error: {response.status_code}\""
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "Pkek8hxY9GXR"
      },
      "source": [
        "Check the robots.txt of each web page, by adjusting range(1751, 3500) \n",
        "-> all data: len(data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6P42tW-9aCY5"
      },
      "outputs": [],
      "source": [
        "for i in range(1751, 3500):\n",
        "  print(i) # only progress information\n",
        "  url = data['website_url'][i]\n",
        "  result = is_scraping_allowed(url)\n",
        "  data.loc[i, \"check_robots\"] = result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "effdqasaaKQs"
      },
      "outputs": [],
      "source": [
        "#Save data as new csv\n",
        "data.to_csv(\"/content/drive/MyDrive/\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VwzIWKCr3X7g"
      },
      "outputs": [],
      "source": [
        "data.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LeEIpCfP3eYU"
      },
      "outputs": [],
      "source": [
        "#Counts True and False values in column \"check_robots\"\n",
        "data[\"check_robots\"].value_counts()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h4dq97-hJ5YI"
      },
      "source": [
        "Gets Text from the allowed websites and saves it as txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gd2z3H5-JW6P"
      },
      "outputs": [],
      "source": [
        "#path for the text files \n",
        "path = \"/content/drive/MyDrive/textfiles/\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U_imzBMQT9hC"
      },
      "outputs": [],
      "source": [
        "for i in range(1751, 3500):\n",
        "  if(data[\"check_robots\"][i]==True):\n",
        "    print(i) # only progress information\n",
        "    url = df_test['website_url'][i]\n",
        "    text = getWebsiteText(url)\n",
        "    lines = text.splitlines()\n",
        "    new_lines = [line for line in lines if line]\n",
        "    new_text = \"\\n\".join(new_lines)\n",
        "    # lowercase + remove everything in name but chars\n",
        "    # because some names contains symbols e.g. \":\" -> would destroy a textfile\n",
        "    name = data[\"name\"][i].lower()\n",
        "    name = re.sub(r\"[^a-z]\", \"\", name)\n",
        "    with open(path+name+\".txt\", \"w\", encoding=\"utf-8\") as f:\n",
        "      f.write(new_text)\n",
        "    "
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
