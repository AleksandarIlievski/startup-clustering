{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "vyZNWSAuZsNr"
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import re\n",
    "from urllib.parse import urlparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "2dXXCEGTa-EW"
   },
   "outputs": [],
   "source": [
    "# path to csv file\n",
    "data = pd.read_csv(\"../startup_list_3501-5000.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "WEmwVKcHbGU_"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>name</th>\n",
       "      <th>state</th>\n",
       "      <th>linkedin_industry</th>\n",
       "      <th>website_url</th>\n",
       "      <th>check_robots</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3501</td>\n",
       "      <td>DeepEyes</td>\n",
       "      <td>Bayern</td>\n",
       "      <td>Industrial &amp; Basic Materials</td>\n",
       "      <td>http://deepeyes.co/</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3502</td>\n",
       "      <td>Instafreight</td>\n",
       "      <td>Berlin</td>\n",
       "      <td>Automotive, Logistics &amp; Mobility</td>\n",
       "      <td>https://www.instafreight.de:443/</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3503</td>\n",
       "      <td>smartUP</td>\n",
       "      <td>Baden-Württemberg</td>\n",
       "      <td>Construction/Real Estate</td>\n",
       "      <td>http://www.smartup-immobilien.de</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3504</td>\n",
       "      <td>getnow</td>\n",
       "      <td>Bayern</td>\n",
       "      <td>Nutrition &amp; Consumables</td>\n",
       "      <td>https://www.getnow.com/</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3505</td>\n",
       "      <td>BeeSonix</td>\n",
       "      <td>Berlin</td>\n",
       "      <td>IT and Communications</td>\n",
       "      <td>http://beesonix.de/</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0          name              state   \n",
       "0        3501      DeepEyes             Bayern  \\\n",
       "1        3502  Instafreight             Berlin   \n",
       "2        3503       smartUP  Baden-Württemberg   \n",
       "3        3504        getnow             Bayern   \n",
       "4        3505      BeeSonix             Berlin   \n",
       "\n",
       "                  linkedin_industry                       website_url   \n",
       "0      Industrial & Basic Materials               http://deepeyes.co/  \\\n",
       "1  Automotive, Logistics & Mobility  https://www.instafreight.de:443/   \n",
       "2          Construction/Real Estate  http://www.smartup-immobilien.de   \n",
       "3           Nutrition & Consumables           https://www.getnow.com/   \n",
       "4             IT and Communications               http://beesonix.de/   \n",
       "\n",
       "   check_robots  \n",
       "0          True  \n",
       "1         False  \n",
       "2          True  \n",
       "3         False  \n",
       "4         False  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "J065garRZ_mW"
   },
   "outputs": [],
   "source": [
    "def is_scraping_allowed(url):\n",
    "    \"\"\"\n",
    "    Checks if scraping is allowed for the given URL.\n",
    "\n",
    "    Args:\n",
    "        url (str): The URL to check.\n",
    "\n",
    "    Returns:\n",
    "        bool: True if scraping is allowed, False otherwise.\n",
    "    \"\"\"\n",
    "    # Parse the URL to extract the domain.\n",
    "    domain = urlparse(url).netloc\n",
    "\n",
    "    # Get the URL for the robots.txt file for the domain.\n",
    "    robots_url = f\"https://{domain}/robots.txt\"\n",
    "\n",
    "    # Send a GET request to the robots.txt file.\n",
    "    try:\n",
    "        response = requests.get(robots_url)\n",
    "    except:\n",
    "        # If there was an error, assume scraping is not allowed.\n",
    "        return False\n",
    "\n",
    "    # Check if the status code of the response is OK.\n",
    "    if response.status_code != 200:\n",
    "        # If not, assume scraping is not allowed.\n",
    "        return False\n",
    "\n",
    "    # Parse the robots.txt file.\n",
    "    robots_txt = response.text\n",
    "\n",
    "    # Check if the user-agent \"*\" is disallowed from scraping any pages.\n",
    "    #if \"User-agent: *\\nDisallow: /\" in robots_txt:\n",
    "    #    return False\n",
    "\n",
    "    # Check if the user-agent \"*\" is specifically disallowed from scraping the given URL.\n",
    "    if f\"User-agent: *\\nDisallow: {url}\" in robots_txt:\n",
    "        return False\n",
    "\n",
    "    # If none of the above conditions are met, assume scraping is allowed.\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "jQ1KAgIaU1kZ"
   },
   "outputs": [],
   "source": [
    "def getWebsiteText(url):\n",
    "    # Headers with Accept-Language set to English\n",
    "    headers = {\n",
    "        'Accept-Language': 'en-US,en;q=0.1',\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3'\n",
    "    }\n",
    "\n",
    "    # Send a request to the website and get the response\n",
    "    try:\n",
    "        response = requests.get(url, headers=headers)\n",
    "        if response.status_code == 200:\n",
    "            #print(f\"The link {url} is working.\")\n",
    "            # Parse the HTML content with BeautifulSoup\n",
    "            soup = BeautifulSoup(response.content, 'html.parser')\n",
    "            text = soup.getText()\n",
    "            return text\n",
    "        else:\n",
    "            return f\"Error: {response.status_code}\"\n",
    "\n",
    "    except Exception as e:\n",
    "        # Handle the error here\n",
    "        if 'response' not in locals():\n",
    "            return \"Error: Failed to get response\"\n",
    "        return f\"Error: {response.status_code}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "\n",
    "def remove_cookie_banners(soup):\n",
    "    # Find and remove elements with classes containing classes_to_remove content\n",
    "    classes_to_remove = ['cookie', 'banner', 'popup', 'modal', \"cc-dialog\", \"gdpr-content\", \"ccform\"]  # Customize as needed\n",
    "    for class_name in classes_to_remove:\n",
    "        elements = soup.select(f'[class*={class_name}]')\n",
    "        for element in elements:\n",
    "            element.extract()\n",
    "\n",
    "    # Find and remove elements with IDs containing cookie-law-info-bar\n",
    "    elements = soup.select('[id*=cookie-law-info-bar]')\n",
    "    for element in elements:\n",
    "        element.extract()\n",
    "\n",
    "    # Find and remove elements with classes or IDs containing classes_to_remove content\n",
    "    pattern = re.compile(r'class=[\"\\'].*?\\bcookie\\b.*?[\"\\']|id=[\"\\'].*?\\bcookie\\b.*?[\"\\']')\n",
    "    tags_with_cookie = soup.find_all(attrs={\"class\": pattern, \"id\": pattern})\n",
    "    for tag in tags_with_cookie:\n",
    "        tag.extract()\n",
    "    # Find and remove elements with classes or IDs containing classes_to_remove content\n",
    "    pattern = re.compile(r'class=[\"\\'].*?\\bcc\\b.*?[\"\\']|id=[\"\\'].*?\\bcc\\b.*?[\"\\']')\n",
    "    tags_with_cookie = soup.find_all(attrs={\"class\": pattern, \"id\": pattern})\n",
    "    for tag in tags_with_cookie:\n",
    "        tag.extract()\n",
    "\n",
    "def getWebsiteText(url):\n",
    "    # Headers with Accept-Language set to English and Cookie consent\n",
    "    headers = {\n",
    "        'Accept-Language': 'en-US,en;q=0.1',\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3',\n",
    "        'Cookie': 'consent=true'  # Add the cookie consent header\n",
    "    }\n",
    "\n",
    "    # Send a request to the website and get the response\n",
    "    try:\n",
    "        response = requests.get(url, headers=headers)\n",
    "        if response.status_code == 200:\n",
    "            # Parse the HTML content with BeautifulSoup\n",
    "            soup = BeautifulSoup(response.content, 'html.parser')\n",
    "            remove_cookie_banners(soup)  # Remove elements with classes or IDs containing specified content\n",
    "            text = soup.get_text()\n",
    "            return text\n",
    "        else:\n",
    "            return f\"Error: {response.status_code}\"\n",
    "\n",
    "    except Exception as e:\n",
    "        # Handle the error here\n",
    "        if 'response' not in locals():\n",
    "            return \"Error: Failed to get response\"\n",
    "        return f\"Error: {response.status_code}\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Pkek8hxY9GXR"
   },
   "source": [
    "Check the robots.txt of each web page, by adjusting range(1751, 3500) \n",
    "-> all data: len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true,
    "id": "6P42tW-9aCY5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1751\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "1751",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "File \u001B[0;32m~/opt/anaconda3/envs/BDA/lib/python3.9/site-packages/pandas/core/indexes/range.py:345\u001B[0m, in \u001B[0;36mRangeIndex.get_loc\u001B[0;34m(self, key)\u001B[0m\n\u001B[1;32m    344\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 345\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_range\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mindex\u001B[49m\u001B[43m(\u001B[49m\u001B[43mnew_key\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    346\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m err:\n",
      "\u001B[0;31mValueError\u001B[0m: 1751 is not in range",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001B[0;31mKeyError\u001B[0m                                  Traceback (most recent call last)",
      "Input \u001B[0;32mIn [40]\u001B[0m, in \u001B[0;36m<cell line: 1>\u001B[0;34m()\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m i \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(\u001B[38;5;241m1751\u001B[39m, \u001B[38;5;241m3500\u001B[39m):\n\u001B[1;32m      2\u001B[0m   \u001B[38;5;28mprint\u001B[39m(i) \u001B[38;5;66;03m# only progress information\u001B[39;00m\n\u001B[0;32m----> 3\u001B[0m   url \u001B[38;5;241m=\u001B[39m \u001B[43mdata\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mwebsite_url\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m[\u001B[49m\u001B[43mi\u001B[49m\u001B[43m]\u001B[49m\n\u001B[1;32m      4\u001B[0m   result \u001B[38;5;241m=\u001B[39m is_scraping_allowed(url)\n\u001B[1;32m      5\u001B[0m   data\u001B[38;5;241m.\u001B[39mloc[i, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcheck_robots\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m result\n",
      "File \u001B[0;32m~/opt/anaconda3/envs/BDA/lib/python3.9/site-packages/pandas/core/series.py:1007\u001B[0m, in \u001B[0;36mSeries.__getitem__\u001B[0;34m(self, key)\u001B[0m\n\u001B[1;32m   1004\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_values[key]\n\u001B[1;32m   1006\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m key_is_scalar:\n\u001B[0;32m-> 1007\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_get_value\u001B[49m\u001B[43m(\u001B[49m\u001B[43mkey\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1009\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m is_hashable(key):\n\u001B[1;32m   1010\u001B[0m     \u001B[38;5;66;03m# Otherwise index.get_value will raise InvalidIndexError\u001B[39;00m\n\u001B[1;32m   1011\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m   1012\u001B[0m         \u001B[38;5;66;03m# For labels that don't resolve as scalars like tuples and frozensets\u001B[39;00m\n",
      "File \u001B[0;32m~/opt/anaconda3/envs/BDA/lib/python3.9/site-packages/pandas/core/series.py:1116\u001B[0m, in \u001B[0;36mSeries._get_value\u001B[0;34m(self, label, takeable)\u001B[0m\n\u001B[1;32m   1113\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_values[label]\n\u001B[1;32m   1115\u001B[0m \u001B[38;5;66;03m# Similar to Index.get_value, but we do not fall back to positional\u001B[39;00m\n\u001B[0;32m-> 1116\u001B[0m loc \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mindex\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget_loc\u001B[49m\u001B[43m(\u001B[49m\u001B[43mlabel\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1118\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m is_integer(loc):\n\u001B[1;32m   1119\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_values[loc]\n",
      "File \u001B[0;32m~/opt/anaconda3/envs/BDA/lib/python3.9/site-packages/pandas/core/indexes/range.py:347\u001B[0m, in \u001B[0;36mRangeIndex.get_loc\u001B[0;34m(self, key)\u001B[0m\n\u001B[1;32m    345\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_range\u001B[38;5;241m.\u001B[39mindex(new_key)\n\u001B[1;32m    346\u001B[0m     \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m err:\n\u001B[0;32m--> 347\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mKeyError\u001B[39;00m(key) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01merr\u001B[39;00m\n\u001B[1;32m    348\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(key, Hashable):\n\u001B[1;32m    349\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mKeyError\u001B[39;00m(key)\n",
      "\u001B[0;31mKeyError\u001B[0m: 1751"
     ]
    }
   ],
   "source": [
    "for i in range(1751, 3500):\n",
    "  print(i) # only progress information\n",
    "  url = data['website_url'][i]\n",
    "  result = is_scraping_allowed(url)\n",
    "  data.loc[i, \"check_robots\"] = result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "effdqasaaKQs"
   },
   "outputs": [],
   "source": [
    "#Save data as new csv\n",
    "data.to_csv(\"/content/drive/MyDrive/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "VwzIWKCr3X7g"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>name</th>\n",
       "      <th>state</th>\n",
       "      <th>linkedin_industry</th>\n",
       "      <th>website_url</th>\n",
       "      <th>check_robots</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3501</td>\n",
       "      <td>DeepEyes</td>\n",
       "      <td>Bayern</td>\n",
       "      <td>Industrial &amp; Basic Materials</td>\n",
       "      <td>http://deepeyes.co/</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3502</td>\n",
       "      <td>Instafreight</td>\n",
       "      <td>Berlin</td>\n",
       "      <td>Automotive, Logistics &amp; Mobility</td>\n",
       "      <td>https://www.instafreight.de:443/</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3503</td>\n",
       "      <td>smartUP</td>\n",
       "      <td>Baden-Württemberg</td>\n",
       "      <td>Construction/Real Estate</td>\n",
       "      <td>http://www.smartup-immobilien.de</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3504</td>\n",
       "      <td>getnow</td>\n",
       "      <td>Bayern</td>\n",
       "      <td>Nutrition &amp; Consumables</td>\n",
       "      <td>https://www.getnow.com/</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3505</td>\n",
       "      <td>BeeSonix</td>\n",
       "      <td>Berlin</td>\n",
       "      <td>IT and Communications</td>\n",
       "      <td>http://beesonix.de/</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0          name              state   \n",
       "0        3501      DeepEyes             Bayern  \\\n",
       "1        3502  Instafreight             Berlin   \n",
       "2        3503       smartUP  Baden-Württemberg   \n",
       "3        3504        getnow             Bayern   \n",
       "4        3505      BeeSonix             Berlin   \n",
       "\n",
       "                  linkedin_industry                       website_url   \n",
       "0      Industrial & Basic Materials               http://deepeyes.co/  \\\n",
       "1  Automotive, Logistics & Mobility  https://www.instafreight.de:443/   \n",
       "2          Construction/Real Estate  http://www.smartup-immobilien.de   \n",
       "3           Nutrition & Consumables           https://www.getnow.com/   \n",
       "4             IT and Communications               http://beesonix.de/   \n",
       "\n",
       "   check_robots  \n",
       "0          True  \n",
       "1         False  \n",
       "2          True  \n",
       "3         False  \n",
       "4         False  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "LeEIpCfP3eYU"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "check_robots\n",
       "True     990\n",
       "False    509\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Counts True and False values in column \"check_robots\"\n",
    "data[\"check_robots\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h4dq97-hJ5YI"
   },
   "source": [
    "Gets Text from the allowed websites and saves it as txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "id": "gd2z3H5-JW6P"
   },
   "outputs": [],
   "source": [
    "#path for the text files \n",
    "import os\n",
    "path = \"./txt_files_3501_5000/\"\n",
    "os.makedirs(path, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "U_imzBMQT9hC",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: deepeyes\n",
      "2: smartup\n",
      "5: surgmark\n",
      "6: rivr\n",
      "7: soundnotation\n",
      "9: fixario\n",
      "10: wix\n",
      "12: zvarmcom\n",
      "14: tibeo\n",
      "15: beemgee\n",
      "17: wechselgott\n",
      "20: creacando\n",
      "21: animatch\n",
      "22: oskarde\n",
      "23: legalhero\n",
      "24: clecta\n",
      "25: maschinenfrholz\n",
      "26: statpile\n",
      "27: eless\n",
      "31: optonlinephysiotherapie\n",
      "32: perun\n",
      "34: dynamicbiosensors\n",
      "35: artmano\n",
      "36: creativequantum\n",
      "37: erento\n",
      "38: commerceconnector\n",
      "39: kaduda\n",
      "40: axelspringerhytechnologies\n",
      "42: timebro\n",
      "44: yoochoose\n",
      "45: cordition\n",
      "48: hometogo\n",
      "49: heavenhr\n",
      "50: virtualretail\n",
      "51: promipool\n",
      "52: reparaturmacherdasvergleichsportalfrreparaturen\n",
      "54: virtonomy\n",
      "57: roestbar\n",
      "59: exocad\n",
      "62: brainscent\n",
      "64: studentengo\n",
      "66: novihumtechnologies\n",
      "67: quadratologo\n",
      "68: kontist\n",
      "70: audibene\n",
      "71: buynomics\n",
      "72: yorxs\n",
      "73: cbddeal\n",
      "74: b\n",
      "76: meintierbestatterde\n",
      "77: rententippsde\n",
      "78: telelook\n",
      "79: nuicosmetics\n",
      "80: instrumentsofthings\n",
      "81: mambu\n",
      "82: applyojena\n",
      "83: y\n",
      "84: betriebsmittelhelden\n",
      "85: kinderheldin\n",
      "86: kartenmacherei\n",
      "87: innokrobotics\n",
      "88: deutscheautohaus\n",
      "89: artflakes\n"
     ]
    }
   ],
   "source": [
    "for i in range(0, 1501):\n",
    "  if(data[\"check_robots\"][i]==True):\n",
    "    url = data['website_url'][i]\n",
    "    text = getWebsiteText(url)\n",
    "    lines = text.splitlines()\n",
    "    new_lines = [line for line in lines if line]\n",
    "    new_text = \"\\n\".join(new_lines)\n",
    "    # lowercase + remove everything in name but chars\n",
    "    # because some names contains symbols e.g. \":\" -> would destroy a textfile\n",
    "    name = data[\"name\"][i].lower()\n",
    "    name = re.sub(r\"[^a-z]\", \"\", name)\n",
    "    print(f\"{i}: {name}\") # only progress information\n",
    "    with open(path+name+\".txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "      f.write(new_text)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}